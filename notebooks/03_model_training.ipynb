{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d99b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# # Home Credit Default Risk - Model Training\n",
    "# \n",
    "# ## ðŸ¤– Overview\n",
    "# This notebook trains machine learning models to predict loan default risk:\n",
    "# 1. Load engineered features\n",
    "# 2. Train multiple models\n",
    "# 3. Evaluate and compare performance\n",
    "# 4. Make predictions on test data\n",
    "# \n",
    "# ## ðŸ“Š Models to Train\n",
    "# 1. Logistic Regression (Baseline)\n",
    "# 2. Random Forest\n",
    "# 3. XGBoost\n",
    "# 4. LightGBM (Recommended)\n",
    "# 5. CatBoost\n",
    "\n",
    "# %% [markdown]\n",
    "# ## ðŸ“¦ Setup and Imports\n",
    "\n",
    "# %%\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Model imports\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, classification_report, confusion_matrix, roc_curve\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import catboost as cb\n",
    "\n",
    "# Import project modules\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "from utils import load_config\n",
    "from model_training import ModelTrainer\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "# Visualization settings\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# %%\n",
    "# Load configuration\n",
    "config = load_config()\n",
    "print(\"Configuration loaded successfully\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## ðŸ“ Load Engineered Features\n",
    "\n",
    "# %%\n",
    "# Load features\n",
    "print(\"Loading engineered features...\")\n",
    "\n",
    "train_path = '../features/train_features.pkl'\n",
    "test_path = '../features/test_features.pkl'\n",
    "\n",
    "try:\n",
    "    train_df = pd.read_pickle(train_path)\n",
    "    test_df = pd.read_pickle(test_path)\n",
    "    \n",
    "    print(f\"Training data shape: {train_df.shape}\")\n",
    "    print(f\"Test data shape: {test_df.shape}\")\n",
    "    \n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"Please run feature engineering notebook first!\")\n",
    "    # If features don't exist, we'll create a simple version\n",
    "    train_df = None\n",
    "    test_df = None\n",
    "\n",
    "# %%\n",
    "if train_df is not None:\n",
    "    # Display basic information\n",
    "    print(\"\\nTraining Data Info:\")\n",
    "    print(f\"Samples: {len(train_df)}\")\n",
    "    print(f\"Features: {len(train_df.columns) - 2}\")  # Excluding SK_ID_CURR and TARGET\n",
    "    \n",
    "    if 'TARGET' in train_df.columns:\n",
    "        target_dist = train_df['TARGET'].value_counts(normalize=True) * 100\n",
    "        print(f\"Target Distribution:\")\n",
    "        print(f\"  0 (No Default): {target_dist[0]:.2f}%\")\n",
    "        print(f\"  1 (Default): {target_dist[1]:.2f}%\")\n",
    "    \n",
    "    # Show feature types\n",
    "    print(f\"\\nData Types:\")\n",
    "    print(train_df.dtypes.value_counts())\n",
    "\n",
    "# %% [markdown]\n",
    "# ## ðŸŽ¯ Prepare Data for Training\n",
    "\n",
    "# %%\n",
    "def prepare_data(df):\n",
    "    \"\"\"Prepare data for model training.\"\"\"\n",
    "    \n",
    "    if df is None or 'TARGET' not in df.columns:\n",
    "        print(\"Error: Training data not available or TARGET column missing\")\n",
    "        return None\n",
    "    \n",
    "    # Separate features and target\n",
    "    X = df.drop(columns=['TARGET', 'SK_ID_CURR'])\n",
    "    y = df['TARGET']\n",
    "    \n",
    "    # Get feature names\n",
    "    feature_names = list(X.columns)\n",
    "    \n",
    "    print(f\"\\nData Preparation:\")\n",
    "    print(f\"  Features: {len(feature_names)}\")\n",
    "    print(f\"  Samples: {len(X)}\")\n",
    "    print(f\"  Positive class: {y.sum()} ({100*y.mean():.2f}%)\")\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X, y, \n",
    "        test_size=config['model']['test_size'],\n",
    "        random_state=config['model']['random_state'],\n",
    "        stratify=y\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nTrain/Validation Split:\")\n",
    "    print(f\"  Training set: {X_train.shape}\")\n",
    "    print(f\"  Validation set: {X_val.shape}\")\n",
    "    print(f\"  Train positive: {y_train.sum()} ({100*y_train.mean():.2f}%)\")\n",
    "    print(f\"  Val positive: {y_val.sum()} ({100*y_val.mean():.2f}%)\")\n",
    "    \n",
    "    return X_train, X_val, y_train, y_val, feature_names\n",
    "\n",
    "# %%\n",
    "# Prepare data\n",
    "data = prepare_data(train_df)\n",
    "\n",
    "if data:\n",
    "    X_train, X_val, y_train, y_val, feature_names = data\n",
    "\n",
    "# %% [markdown]\n",
    "# ## ðŸ¤– Train Logistic Regression (Baseline)\n",
    "\n",
    "# %%\n",
    "def train_logistic_regression(X_train, y_train, X_val, y_val):\n",
    "    \"\"\"Train and evaluate logistic regression model.\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Training Logistic Regression\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Initialize model\n",
    "    model = LogisticRegression(\n",
    "        random_state=config['model']['random_state'],\n",
    "        class_weight='balanced',\n",
    "        max_iter=1000,\n",
    "        solver='liblinear',\n",
    "        C=0.1  # Regularization strength\n",
    "    )\n",
    "    \n",
    "    # Train model\n",
    "    print(\"Training model...\")\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred_proba = model.predict_proba(X_val)[:, 1]\n",
    "    y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    auc = roc_auc_score(y_val, y_pred_proba)\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    \n",
    "    print(f\"\\nValidation Results:\")\n",
    "    print(f\"  AUC Score: {auc:.4f}\")\n",
    "    print(f\"  Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    # Classification report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_val, y_pred, target_names=['Non-Default', 'Default']))\n",
    "    \n",
    "    # Cross-validation\n",
    "    cv_scores = cross_val_score(\n",
    "        model, pd.concat([X_train, X_val]), pd.concat([y_train, y_val]),\n",
    "        cv=5, scoring='roc_auc', n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nCross-Validation (5-fold):\")\n",
    "    print(f\"  AUC Scores: {cv_scores}\")\n",
    "    print(f\"  Mean AUC: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\n",
    "    \n",
    "    return model, y_pred_proba, y_pred, auc, accuracy\n",
    "\n",
    "# %%\n",
    "# Train logistic regression\n",
    "if data:\n",
    "    lr_model, lr_proba, lr_pred, lr_auc, lr_acc = train_logistic_regression(X_train, y_train, X_val, y_val)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## ðŸŒ² Train Random Forest\n",
    "\n",
    "# %%\n",
    "def train_random_forest(X_train, y_train, X_val, y_val):\n",
    "    \"\"\"Train and evaluate random forest model.\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Training Random Forest\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Initialize model\n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=10,\n",
    "        min_samples_split=10,\n",
    "        min_samples_leaf=5,\n",
    "        class_weight='balanced',\n",
    "        random_state=config['model']['random_state'],\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    # Train model\n",
    "    print(\"Training model...\")\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred_proba = model.predict_proba(X_val)[:, 1]\n",
    "    y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    auc = roc_auc_score(y_val, y_pred_proba)\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    \n",
    "    print(f\"\\nValidation Results:\")\n",
    "    print(f\"  AUC Score: {auc:.4f}\")\n",
    "    print(f\"  Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    # Feature importance\n",
    "    importance = pd.DataFrame({\n",
    "        'feature': X_train.columns,\n",
    "        'importance': model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(f\"\\nTop 10 Important Features:\")\n",
    "    for i, row in importance.head(10).iterrows():\n",
    "        print(f\"  {row['feature']}: {row['importance']:.4f}\")\n",
    "    \n",
    "    return model, y_pred_proba, y_pred, auc, accuracy, importance\n",
    "\n",
    "# %%\n",
    "# Train random forest\n",
    "if data:\n",
    "    rf_model, rf_proba, rf_pred, rf_auc, rf_acc, rf_importance = train_random_forest(X_train, y_train, X_val, y_val)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## âš¡ Train XGBoost\n",
    "\n",
    "# %%\n",
    "def train_xgboost(X_train, y_train, X_val, y_val):\n",
    "    \"\"\"Train and evaluate XGBoost model.\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Training XGBoost\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Get hyperparameters from config\n",
    "    params = config['hyperparameters']['xgboost']\n",
    "    \n",
    "    # Initialize model\n",
    "    model = xgb.XGBClassifier(\n",
    "        n_estimators=params['n_estimators'],\n",
    "        learning_rate=params['learning_rate'],\n",
    "        max_depth=params['max_depth'],\n",
    "        subsample=params['subsample'],\n",
    "        colsample_bytree=params['colsample_bytree'],\n",
    "        random_state=config['model']['random_state'],\n",
    "        n_jobs=-1,\n",
    "        eval_metric='auc',\n",
    "        use_label_encoder=False,\n",
    "        scale_pos_weight=len(y_train[y_train==0]) / len(y_train[y_train==1])  # Handle imbalance\n",
    "    )\n",
    "    \n",
    "    # Train model\n",
    "    print(\"Training model...\")\n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=[(X_val, y_val)],\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred_proba = model.predict_proba(X_val)[:, 1]\n",
    "    y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    auc = roc_auc_score(y_val, y_pred_proba)\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    \n",
    "    print(f\"\\nValidation Results:\")\n",
    "    print(f\"  AUC Score: {auc:.4f}\")\n",
    "    print(f\"  Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    # Feature importance\n",
    "    importance = pd.DataFrame({\n",
    "        'feature': X_train.columns,\n",
    "        'importance': model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(f\"\\nTop 10 Important Features:\")\n",
    "    for i, row in importance.head(10).iterrows():\n",
    "        print(f\"  {row['feature']}: {row['importance']:.4f}\")\n",
    "    \n",
    "    # Plot learning curve\n",
    "    results = model.evals_result()\n",
    "    epochs = len(results['validation_0']['auc'])\n",
    "    x_axis = range(0, epochs)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    ax.plot(x_axis, results['validation_0']['auc'], label='Train')\n",
    "    ax.legend()\n",
    "    plt.ylabel('AUC')\n",
    "    plt.title('XGBoost AUC over Iterations')\n",
    "    plt.show()\n",
    "    \n",
    "    return model, y_pred_proba, y_pred, auc, accuracy, importance\n",
    "\n",
    "# %%\n",
    "# Train XGBoost\n",
    "if data:\n",
    "    xgb_model, xgb_proba, xgb_pred, xgb_auc, xgb_acc, xgb_importance = train_xgboost(X_train, y_train, X_val, y_val)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## ðŸ’¡ Train LightGBM\n",
    "\n",
    "# %%\n",
    "def train_lightgbm(X_train, y_train, X_val, y_val):\n",
    "    \"\"\"Train and evaluate LightGBM model.\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Training LightGBM\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Get hyperparameters from config\n",
    "    params = config['hyperparameters']['lightgbm']\n",
    "    \n",
    "    # Initialize model\n",
    "    model = lgb.LGBMClassifier(\n",
    "        n_estimators=params['n_estimators'],\n",
    "        learning_rate=params['learning_rate'],\n",
    "        max_depth=params['max_depth'],\n",
    "        num_leaves=params['num_leaves'],\n",
    "        subsample=params['subsample'],\n",
    "        colsample_bytree=params['colsample_bytree'],\n",
    "        reg_alpha=params['reg_alpha'],\n",
    "        reg_lambda=params['reg_lambda'],\n",
    "        random_state=config['model']['random_state'],\n",
    "        n_jobs=-1,\n",
    "        class_weight='balanced'\n",
    "    )\n",
    "    \n",
    "    # Train model\n",
    "    print(\"Training model...\")\n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=[(X_val, y_val)],\n",
    "        eval_metric='auc',\n",
    "        callbacks=[\n",
    "            lgb.early_stopping(50),\n",
    "            lgb.log_evaluation(50)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred_proba = model.predict_proba(X_val)[:, 1]\n",
    "    y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    auc = roc_auc_score(y_val, y_pred_proba)\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    \n",
    "    print(f\"\\nValidation Results:\")\n",
    "    print(f\"  AUC Score: {auc:.4f}\")\n",
    "    print(f\"  Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    # Feature importance\n",
    "    importance = pd.DataFrame({\n",
    "        'feature': X_train.columns,\n",
    "        'importance': model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(f\"\\nTop 10 Important Features:\")\n",
    "    for i, row in importance.head(10).iterrows():\n",
    "        print(f\"  {row['feature']}: {row['importance']:.4f}\")\n",
    "    \n",
    "    # Plot feature importance\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    lgb.plot_importance(model, max_num_features=20, importance_type='gain')\n",
    "    plt.title('LightGBM Feature Importance')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return model, y_pred_proba, y_pred, auc, accuracy, importance\n",
    "\n",
    "# %%\n",
    "# Train LightGBM\n",
    "if data:\n",
    "    lgb_model, lgb_proba, lgb_pred, lgb_auc, lgb_acc, lgb_importance = train_lightgbm(X_train, y_train, X_val, y_val)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## ðŸ± Train CatBoost\n",
    "\n",
    "# %%\n",
    "def train_catboost(X_train, y_train, X_val, y_val):\n",
    "    \"\"\"Train and evaluate CatBoost model.\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Training CatBoost\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Get hyperparameters from config\n",
    "    params = config['hyperparameters']['catboost']\n",
    "    \n",
    "    # Identify categorical columns\n",
    "    categorical_features = X_train.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    \n",
    "    # Initialize model\n",
    "    model = cb.CatBoostClassifier(\n",
    "        iterations=params['iterations'],\n",
    "        learning_rate=params['learning_rate'],\n",
    "        depth=params['depth'],\n",
    "        l2_leaf_reg=params['l2_leaf_reg'],\n",
    "        random_state=config['model']['random_state'],\n",
    "        verbose=100,\n",
    "        cat_features=categorical_features,\n",
    "        auto_class_weights='Balanced'\n",
    "    )\n",
    "    \n",
    "    # Train model\n",
    "    print(\"Training model...\")\n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=(X_val, y_val),\n",
    "        early_stopping_rounds=50,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred_proba = model.predict_proba(X_val)[:, 1]\n",
    "    y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    auc = roc_auc_score(y_val, y_pred_proba)\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    \n",
    "    print(f\"\\nValidation Results:\")\n",
    "    print(f\"  AUC Score: {auc:.4f}\")\n",
    "    print(f\"  Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    # Feature importance\n",
    "    importance = pd.DataFrame({\n",
    "        'feature': X_train.columns,\n",
    "        'importance': model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    print(f\"\\nTop 10 Important Features:\")\n",
    "    for i, row in importance.head(10).iterrows():\n",
    "        print(f\"  {row['feature']}: {row['importance']:.4f}\")\n",
    "    \n",
    "    return model, y_pred_proba, y_pred, auc, accuracy, importance\n",
    "\n",
    "# %%\n",
    "# Train CatBoost\n",
    "if data:\n",
    "    cb_model, cb_proba, cb_pred, cb_auc, cb_acc, cb_importance = train_catboost(X_train, y_train, X_val, y_val)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## ðŸ“Š Model Comparison\n",
    "\n",
    "# %%\n",
    "def compare_models(model_results):\n",
    "    \"\"\"Compare performance of all trained models.\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Model Comparison\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Create comparison DataFrame\n",
    "    comparison_df = pd.DataFrame(model_results).T\n",
    "    comparison_df = comparison_df[['auc', 'accuracy']]\n",
    "    \n",
    "    print(\"\\nModel Performance Summary:\")\n",
    "    print(comparison_df.sort_values('auc', ascending=False))\n",
    "    \n",
    "    # Visual comparison\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # 1. AUC Comparison\n",
    "    axes[0, 0].barh(range(len(comparison_df)), comparison_df['auc'].sort_values())\n",
    "    axes[0, 0].set_yticks(range(len(comparison_df)))\n",
    "    axes[0, 0].set_yticklabels(comparison_df['auc'].sort_values().index)\n",
    "    axes[0, 0].set_xlabel('AUC Score')\n",
    "    axes[0, 0].set_title('Model AUC Comparison')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Accuracy Comparison\n",
    "    axes[0, 1].barh(range(len(comparison_df)), comparison_df['accuracy'].sort_values())\n",
    "    axes[0, 1].set_yticks(range(len(comparison_df)))\n",
    "    axes[0, 1].set_yticklabels(comparison_df['accuracy'].sort_values().index)\n",
    "    axes[0, 1].set_xlabel('Accuracy')\n",
    "    axes[0, 1].set_title('Model Accuracy Comparison')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. ROC Curves\n",
    "    axes[1, 0].set_title('ROC Curves Comparison')\n",
    "    axes[1, 0].set_xlabel('False Positive Rate')\n",
    "    axes[1, 0].set_ylabel('True Positive Rate')\n",
    "    \n",
    "    for model_name, results in model_results.items():\n",
    "        if 'proba' in results:\n",
    "            fpr, tpr, _ = roc_curve(y_val, results['proba'])\n",
    "            axes[1, 0].plot(fpr, tpr, label=f'{model_name} (AUC = {results[\"auc\"]:.3f})')\n",
    "    \n",
    "    axes[1, 0].plot([0, 1], [0, 1], 'k--', label='Random')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Feature Importance Comparison (for tree-based models)\n",
    "    tree_models = ['random_forest', 'xgboost', 'lightgbm', 'catboost']\n",
    "    importance_data = {}\n",
    "    \n",
    "    for model_name in tree_models:\n",
    "        if model_name in model_results and 'importance' in model_results[model_name]:\n",
    "            importance_data[model_name] = model_results[model_name]['importance'].head(10)\n",
    "    \n",
    "    # Create a combined importance plot\n",
    "    if importance_data:\n",
    "        all_features = set()\n",
    "        for imp_df in importance_data.values():\n",
    "            all_features.update(imp_df['feature'].head(5).tolist())\n",
    "        \n",
    "        # Create a DataFrame for the plot\n",
    "        plot_data = pd.DataFrame(index=list(all_features))\n",
    "        \n",
    "        for model_name, imp_df in importance_data.items():\n",
    "            top_features = imp_df.set_index('feature')['importance'].head(5)\n",
    "            plot_data[model_name] = top_features\n",
    "        \n",
    "        plot_data = plot_data.fillna(0)\n",
    "        plot_data.plot(kind='bar', ax=axes[1, 1])\n",
    "        axes[1, 1].set_title('Top Feature Importance Comparison')\n",
    "        axes[1, 1].set_xlabel('Feature')\n",
    "        axes[1, 1].set_ylabel('Importance')\n",
    "        axes[1, 1].legend()\n",
    "        axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Select best model\n",
    "    best_model_name = comparison_df['auc'].idxmax()\n",
    "    best_auc = comparison_df.loc[best_model_name, 'auc']\n",
    "    \n",
    "    print(f\"\\nBest Model: {best_model_name} (AUC: {best_auc:.4f})\")\n",
    "    \n",
    "    return best_model_name, best_auc\n",
    "\n",
    "# %%\n",
    "# Collect model results\n",
    "if data:\n",
    "    model_results = {\n",
    "        'logistic_regression': {\n",
    "            'auc': lr_auc,\n",
    "            'accuracy': lr_acc,\n",
    "            'proba': lr_proba,\n",
    "            'pred': lr_pred\n",
    "        },\n",
    "        'random_forest': {\n",
    "            'auc': rf_auc,\n",
    "            'accuracy': rf_acc,\n",
    "            'proba': rf_proba,\n",
    "            'pred': rf_pred,\n",
    "            'importance': rf_importance\n",
    "        },\n",
    "        'xgboost': {\n",
    "            'auc': xgb_auc,\n",
    "            'accuracy': xgb_acc,\n",
    "            'proba': xgb_proba,\n",
    "            'pred': xgb_pred,\n",
    "            'importance': xgb_importance\n",
    "        },\n",
    "        'lightgbm': {\n",
    "            'auc': lgb_auc,\n",
    "            'accuracy': lgb_acc,\n",
    "            'proba': lgb_proba,\n",
    "            'pred': lgb_pred,\n",
    "            'importance': lgb_importance\n",
    "        },\n",
    "        'catboost': {\n",
    "            'auc': cb_auc,\n",
    "            'accuracy': cb_acc,\n",
    "            'proba': cb_proba,\n",
    "            'pred': cb_pred,\n",
    "            'importance': cb_importance\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Compare models\n",
    "    best_model_name, best_auc = compare_models(model_results)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## ðŸ’¾ Save Best Model\n",
    "\n",
    "# %%\n",
    "def save_best_model(model, model_name, importance=None):\n",
    "    \"\"\"Save the best model to disk.\"\"\"\n",
    "    \n",
    "    print(f\"\\nSaving best model: {model_name}\")\n",
    "    \n",
    "    import pickle\n",
    "    import os\n",
    "    \n",
    "    # Create models directory\n",
    "    os.makedirs('../models', exist_ok=True)\n",
    "    \n",
    "    # Save model\n",
    "    model_path = f'../models/best_model_{model_name}.pkl'\n",
    "    with open(model_path, 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "    \n",
    "    print(f\"Model saved to: {model_path}\")\n",
    "    \n",
    "    # Save feature importance if available\n",
    "    if importance is not None:\n",
    "        importance_path = f'../models/feature_importance_{model_name}.pkl'\n",
    "        with open(importance_path, 'wb') as f:\n",
    "            pickle.dump(importance, f)\n",
    "        print(f\"Feature importance saved to: {importance_path}\")\n",
    "    \n",
    "    # Also save as best_model.pkl (for easy loading)\n",
    "    best_model_path = '../models/best_model.pkl'\n",
    "    with open(best_model_path, 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "    \n",
    "    print(f\"Best model also saved to: {best_model_path}\")\n",
    "    \n",
    "    return model_path\n",
    "\n",
    "# %%\n",
    "# Save best model\n",
    "if data:\n",
    "    # Determine which model object to save based on best_model_name\n",
    "    model_objects = {\n",
    "        'logistic_regression': lr_model,\n",
    "        'random_forest': rf_model,\n",
    "        'xgboost': xgb_model,\n",
    "        'lightgbm': lgb_model,\n",
    "        'catboost': cb_model\n",
    "    }\n",
    "    \n",
    "    importance_objects = {\n",
    "        'random_forest': rf_importance,\n",
    "        'xgboost': xgb_importance,\n",
    "        'lightgbm': lgb_importance,\n",
    "        'catboost': cb_importance\n",
    "    }\n",
    "    \n",
    "    best_model = model_objects[best_model_name]\n",
    "    best_importance = importance_objects.get(best_model_name)\n",
    "    \n",
    "    saved_path = save_best_model(best_model, best_model_name, best_importance)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## ðŸ”® Make Predictions on Test Data\n",
    "\n",
    "# %%\n",
    "def make_predictions(model, test_df, threshold=0.5):\n",
    "    \"\"\"Make predictions on test data.\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Making Predictions on Test Data\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    if test_df is"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
