{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9241bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# # Home Credit Default Risk - Feature Engineering\n",
    "# \n",
    "# ## ðŸ› ï¸ Overview\n",
    "# This notebook creates new features by:\n",
    "# 1. Aggregating data from multiple sources\n",
    "# 2. Creating interaction features\n",
    "# 3. Handling categorical variables\n",
    "# 4. Creating domain-specific features\n",
    "# \n",
    "# ## ðŸ“Š Data Sources Used\n",
    "# 1. application_train/test.csv (main data)\n",
    "# 2. bureau.csv (external loans)\n",
    "# 3. previous_application.csv (previous Home Credit loans)\n",
    "# 4. Other tables for payment behavior\n",
    "\n",
    "# %% [markdown]\n",
    "# ## ðŸ“¦ Setup and Imports\n",
    "\n",
    "# %%\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import project modules\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "from utils import load_config, load_data, reduce_memory_usage\n",
    "from feature_engineering import FeatureEngineer\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "# %%\n",
    "# Load configuration\n",
    "config = load_config()\n",
    "print(\"Configuration loaded successfully\")\n",
    "\n",
    "# %%\n",
    "# Initialize feature engineer\n",
    "engineer = FeatureEngineer()\n",
    "\n",
    "# %% [markdown]\n",
    "# ## ðŸ“ Load Data\n",
    "\n",
    "# %%\n",
    "# Load main application data\n",
    "print(\"Loading application data...\")\n",
    "app_train = load_data(config['files']['application_train'])\n",
    "app_test = load_data(config['files']['application_test'])\n",
    "\n",
    "print(f\"Training data shape: {app_train.shape}\")\n",
    "print(f\"Test data shape: {app_test.shape}\")\n",
    "\n",
    "# %%\n",
    "# Load bureau data\n",
    "print(\"\\nLoading bureau data...\")\n",
    "bureau = load_data(config['files']['bureau'])\n",
    "\n",
    "print(f\"Bureau data shape: {bureau.shape}\")\n",
    "print(f\"Unique clients in bureau: {bureau['SK_ID_CURR'].nunique()}\")\n",
    "print(f\"Unique loans in bureau: {bureau['SK_ID_BUREAU'].nunique()}\")\n",
    "\n",
    "# %%\n",
    "# Load previous application data\n",
    "print(\"\\nLoading previous application data...\")\n",
    "prev_app = load_data(config['files']['previous_application'])\n",
    "\n",
    "print(f\"Previous application data shape: {prev_app.shape}\")\n",
    "print(f\"Unique clients in previous apps: {prev_app['SK_ID_CURR'].nunique()}\")\n",
    "print(f\"Unique previous applications: {prev_app['SK_ID_PREV'].nunique()}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## ðŸŽ¯ Create Application Features\n",
    "\n",
    "# %%\n",
    "def create_application_features(df):\n",
    "    \"\"\"Create features from main application data.\"\"\"\n",
    "    features_df = df.copy()\n",
    "    \n",
    "    print(f\"Original columns: {len(features_df.columns)}\")\n",
    "    \n",
    "    # 1. Income-related features\n",
    "    if 'AMT_INCOME_TOTAL' in df.columns:\n",
    "        # Income to credit ratio\n",
    "        if 'AMT_CREDIT' in df.columns:\n",
    "            features_df['INCOME_CREDIT_RATIO'] = df['AMT_INCOME_TOTAL'] / (df['AMT_CREDIT'] + 1)\n",
    "            features_df['INCOME_CREDIT_RATIO'].replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "        \n",
    "        # Income per person\n",
    "        if 'CNT_FAM_MEMBERS' in df.columns:\n",
    "            features_df['INCOME_PER_PERSON'] = df['AMT_INCOME_TOTAL'] / (df['CNT_FAM_MEMBERS'] + 1)\n",
    "        \n",
    "        # Income to annuity ratio\n",
    "        if 'AMT_ANNUITY' in df.columns:\n",
    "            features_df['INCOME_ANNUITY_RATIO'] = df['AMT_INCOME_TOTAL'] / (df['AMT_ANNUITY'] + 1)\n",
    "            features_df['INCOME_ANNUITY_RATIO'].replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    \n",
    "    # 2. Credit-related features\n",
    "    if 'AMT_CREDIT' in df.columns:\n",
    "        # Credit to goods price ratio\n",
    "        if 'AMT_GOODS_PRICE' in df.columns:\n",
    "            features_df['CREDIT_GOODS_RATIO'] = df['AMT_CREDIT'] / (df['AMT_GOODS_PRICE'] + 1)\n",
    "        \n",
    "        # Credit to income ratio (already have inverse)\n",
    "        features_df['CREDIT_INCOME_RATIO'] = df['AMT_CREDIT'] / (df['AMT_INCOME_TOTAL'] + 1)\n",
    "    \n",
    "    # 3. Age-related features\n",
    "    if 'DAYS_BIRTH' in df.columns:\n",
    "        features_df['AGE'] = -df['DAYS_BIRTH'] / 365.25\n",
    "        features_df['AGE'] = features_df['AGE'].round(0)\n",
    "        \n",
    "        # Age groups\n",
    "        features_df['AGE_GROUP'] = pd.cut(\n",
    "            features_df['AGE'],\n",
    "            bins=[0, 25, 35, 45, 55, 65, 100],\n",
    "            labels=['18-25', '26-35', '36-45', '46-55', '56-65', '65+']\n",
    "        )\n",
    "    \n",
    "    # 4. Employment features\n",
    "    if 'DAYS_EMPLOYED' in df.columns:\n",
    "        # Flag for unrealistic employment days\n",
    "        features_df['DAYS_EMPLOYED_ANOMALY'] = (df['DAYS_EMPLOYED'] == 365243).astype(int)\n",
    "        \n",
    "        # Employment length in years\n",
    "        features_df['YEARS_EMPLOYED'] = -df['DAYS_EMPLOYED'].replace(365243, np.nan) / 365.25\n",
    "        \n",
    "        # Employment status\n",
    "        features_df['EMPLOYED'] = (features_df['YEARS_EMPLOYED'] > 0).astype(int)\n",
    "    \n",
    "    # 5. Document features\n",
    "    doc_cols = [col for col in df.columns if 'FLAG_DOCUMENT' in col]\n",
    "    if doc_cols:\n",
    "        features_df['DOCUMENT_COUNT'] = df[doc_cols].sum(axis=1)\n",
    "        features_df['HAS_DOCUMENTS'] = (features_df['DOCUMENT_COUNT'] > 0).astype(int)\n",
    "    \n",
    "    # 6. Contact features\n",
    "    contact_cols = ['FLAG_MOBIL', 'FLAG_EMP_PHONE', 'FLAG_WORK_PHONE', \n",
    "                   'FLAG_CONT_MOBILE', 'FLAG_PHONE', 'FLAG_EMAIL']\n",
    "    contact_cols = [col for col in contact_cols if col in df.columns]\n",
    "    if contact_cols:\n",
    "        features_df['CONTACT_INFO_COUNT'] = df[contact_cols].sum(axis=1)\n",
    "        features_df['HAS_CONTACT_INFO'] = (features_df['CONTACT_INFO_COUNT'] > 0).astype(int)\n",
    "    \n",
    "    # 7. Car and property ownership\n",
    "    if 'FLAG_OWN_CAR' in df.columns:\n",
    "        features_df['OWNS_CAR'] = (df['FLAG_OWN_CAR'] == 'Y').astype(int)\n",
    "    \n",
    "    if 'FLAG_OWN_REALTY' in df.columns:\n",
    "        features_df['OWNS_PROPERTY'] = (df['FLAG_OWN_REALTY'] == 'Y').astype(int)\n",
    "    \n",
    "    # 8. Family features\n",
    "    if 'CNT_CHILDREN' in df.columns:\n",
    "        features_df['HAS_CHILDREN'] = (df['CNT_CHILDREN'] > 0).astype(int)\n",
    "    \n",
    "    if 'CNT_FAM_MEMBERS' in df.columns and 'CNT_CHILDREN' in df.columns:\n",
    "        features_df['ADULTS_IN_FAMILY'] = df['CNT_FAM_MEMBERS'] - df['CNT_CHILDREN']\n",
    "    \n",
    "    print(f\"New columns: {len(features_df.columns)}\")\n",
    "    print(f\"Features created: {len(features_df.columns) - len(df.columns)}\")\n",
    "    \n",
    "    return features_df\n",
    "\n",
    "# %%\n",
    "# Create application features\n",
    "print(\"Creating application features for training data...\")\n",
    "app_train_features = create_application_features(app_train)\n",
    "\n",
    "print(\"\\nCreating application features for test data...\")\n",
    "app_test_features = create_application_features(app_test)\n",
    "\n",
    "# %%\n",
    "# Show new features\n",
    "new_features = [col for col in app_train_features.columns if col not in app_train.columns]\n",
    "print(f\"\\nNew application features created: {len(new_features)}\")\n",
    "print(\"Top 10 new features:\")\n",
    "for feature in new_features[:10]:\n",
    "    print(f\"  - {feature}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## ðŸ¦ Create Bureau Features\n",
    "\n",
    "# %%\n",
    "def create_bureau_features(application_df, bureau_df):\n",
    "    \"\"\"Create features from bureau data.\"\"\"\n",
    "    \n",
    "    print(f\"\\nProcessing {len(bureau_df):,} bureau records...\")\n",
    "    \n",
    "    # Create a copy to avoid modifying original\n",
    "    bureau = bureau_df.copy()\n",
    "    \n",
    "    # Initialize aggregations dictionary\n",
    "    bureau_agg = {}\n",
    "    \n",
    "    # 1. Basic counts\n",
    "    bureau_agg['BUREAU_LOAN_COUNT'] = bureau.groupby('SK_ID_CURR')['SK_ID_BUREAU'].count()\n",
    "    bureau_agg['BUREAU_ACTIVE_COUNT'] = bureau[bureau['CREDIT_ACTIVE'] == 'Active'].groupby('SK_ID_CURR').size()\n",
    "    bureau_agg['BUREAU_CLOSED_COUNT'] = bureau[bureau['CREDIT_ACTIVE'] == 'Closed'].groupby('SK_ID_CURR').size()\n",
    "    \n",
    "    # 2. Credit amount aggregations\n",
    "    if 'AMT_CREDIT_SUM' in bureau.columns:\n",
    "        bureau_agg.update({\n",
    "            'BUREAU_CREDIT_SUM_MEAN': bureau.groupby('SK_ID_CURR')['AMT_CREDIT_SUM'].mean(),\n",
    "            'BUREAU_CREDIT_SUM_MAX': bureau.groupby('SK_ID_CURR')['AMT_CREDIT_SUM'].max(),\n",
    "            'BUREAU_CREDIT_SUM_MIN': bureau.groupby('SK_ID_CURR')['AMT_CREDIT_SUM'].min(),\n",
    "            'BUREAU_CREDIT_SUM_SUM': bureau.groupby('SK_ID_CURR')['AMT_CREDIT_SUM'].sum(),\n",
    "        })\n",
    "    \n",
    "    # 3. Debt aggregations\n",
    "    if 'AMT_CREDIT_SUM_DEBT' in bureau.columns:\n",
    "        bureau_agg.update({\n",
    "            'BUREAU_DEBT_MEAN': bureau.groupby('SK_ID_CURR')['AMT_CREDIT_SUM_DEBT'].mean(),\n",
    "            'BUREAU_DEBT_MAX': bureau.groupby('SK_ID_CURR')['AMT_CREDIT_SUM_DEBT'].max(),\n",
    "            'BUREAU_DEBT_SUM': bureau.groupby('SK_ID_CURR')['AMT_CREDIT_SUM_DEBT'].sum(),\n",
    "        })\n",
    "        \n",
    "        # Debt to credit ratio\n",
    "        if 'AMT_CREDIT_SUM' in bureau.columns:\n",
    "            bureau['DEBT_CREDIT_RATIO'] = bureau['AMT_CREDIT_SUM_DEBT'] / (bureau['AMT_CREDIT_SUM'] + 1)\n",
    "            bureau_agg['BUREAU_DEBT_CREDIT_RATIO_MEAN'] = bureau.groupby('SK_ID_CURR')['DEBT_CREDIT_RATIO'].mean()\n",
    "    \n",
    "    # 4. Overdue days\n",
    "    if 'CREDIT_DAY_OVERDUE' in bureau.columns:\n",
    "        bureau_agg.update({\n",
    "            'BUREAU_MAX_OVERDUE_DAYS': bureau.groupby('SK_ID_CURR')['CREDIT_DAY_OVERDUE'].max(),\n",
    "            'BUREAU_MEAN_OVERDUE_DAYS': bureau.groupby('SK_ID_CURR')['CREDIT_DAY_OVERDUE'].mean(),\n",
    "        })\n",
    "        \n",
    "        # Flag for any overdue\n",
    "        bureau['HAS_OVERDUE'] = (bureau['CREDIT_DAY_OVERDUE'] > 0).astype(int)\n",
    "        bureau_agg['BUREAU_HAS_OVERDUE_COUNT'] = bureau.groupby('SK_ID_CURR')['HAS_OVERDUE'].sum()\n",
    "    \n",
    "    # 5. Credit duration\n",
    "    if 'DAYS_CREDIT' in bureau.columns:\n",
    "        bureau_agg.update({\n",
    "            'BUREAU_CREDIT_DAYS_MEAN': bureau.groupby('SK_ID_CURR')['DAYS_CREDIT'].mean(),\n",
    "            'BUREAU_CREDIT_DAYS_MAX': bureau.groupby('SK_ID_CURR')['DAYS_CREDIT'].max(),\n",
    "            'BUREAU_CREDIT_DAYS_MIN': bureau.groupby('SK_ID_CURR')['DAYS_CREDIT'].min(),\n",
    "        })\n",
    "    \n",
    "    # 6. Credit type diversity\n",
    "    if 'CREDIT_TYPE' in bureau.columns:\n",
    "        bureau_agg['BUREAU_CREDIT_TYPES_COUNT'] = bureau.groupby('SK_ID_CURR')['CREDIT_TYPE'].nunique()\n",
    "        \n",
    "        # Most common credit type\n",
    "        most_common = bureau.groupby(['SK_ID_CURR', 'CREDIT_TYPE']).size().reset_index(name='count')\n",
    "        most_common = most_common.loc[most_common.groupby('SK_ID_CURR')['count'].idxmax()]\n",
    "        most_common = most_common.set_index('SK_ID_CURR')['CREDIT_TYPE']\n",
    "        bureau_agg['BUREAU_MAIN_CREDIT_TYPE'] = most_common\n",
    "    \n",
    "    # 7. Currency diversity\n",
    "    if 'CREDIT_CURRENCY' in bureau.columns:\n",
    "        bureau_agg['BUREAU_CURRENCY_COUNT'] = bureau.groupby('SK_ID_CURR')['CREDIT_CURRENCY'].nunique()\n",
    "    \n",
    "    # Create aggregated DataFrame\n",
    "    bureau_features = pd.DataFrame(bureau_agg).reset_index()\n",
    "    \n",
    "    print(f\"Created {len(bureau_features.columns) - 1} bureau features\")\n",
    "    \n",
    "    # Merge with application data\n",
    "    result_df = application_df.merge(bureau_features, on='SK_ID_CURR', how='left')\n",
    "    \n",
    "    # Fill missing values for applicants with no bureau data\n",
    "    bureau_cols = [col for col in bureau_features.columns if col != 'SK_ID_CURR']\n",
    "    for col in bureau_cols:\n",
    "        if col in result_df.columns:\n",
    "            if result_df[col].dtype in ['float64', 'float32', 'float16']:\n",
    "                result_df[col] = result_df[col].fillna(0)\n",
    "            elif result_df[col].dtype == 'object':\n",
    "                result_df[col] = result_df[col].fillna('No Bureau Data')\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "# %%\n",
    "# Create bureau features\n",
    "print(\"Creating bureau features for training data...\")\n",
    "train_with_bureau = create_bureau_features(app_train_features, bureau)\n",
    "\n",
    "print(\"\\nCreating bureau features for test data...\")\n",
    "test_with_bureau = create_bureau_features(app_test_features, bureau)\n",
    "\n",
    "# %%\n",
    "# Show new bureau features\n",
    "bureau_features = [col for col in train_with_bureau.columns if col not in app_train_features.columns]\n",
    "print(f\"\\nNew bureau features created: {len(bureau_features)}\")\n",
    "print(\"Sample bureau features:\")\n",
    "for feature in bureau_features[:10]:\n",
    "    print(f\"  - {feature}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## ðŸ  Create Previous Application Features\n",
    "\n",
    "# %%\n",
    "def create_previous_application_features(application_df, prev_app_df):\n",
    "    \"\"\"Create features from previous Home Credit applications.\"\"\"\n",
    "    \n",
    "    print(f\"\\nProcessing {len(prev_app_df):,} previous applications...\")\n",
    "    \n",
    "    prev = prev_app_df.copy()\n",
    "    \n",
    "    # Initialize aggregations\n",
    "    prev_agg = {}\n",
    "    \n",
    "    # 1. Basic counts\n",
    "    prev_agg['PREV_APPLICATION_COUNT'] = prev.groupby('SK_ID_CURR')['SK_ID_PREV'].count()\n",
    "    \n",
    "    # 2. Application status features\n",
    "    if 'NAME_CONTRACT_STATUS' in prev.columns:\n",
    "        # Count by status\n",
    "        for status in prev['NAME_CONTRACT_STATUS'].unique():\n",
    "            status_count = prev[prev['NAME_CONTRACT_STATUS'] == status].groupby('SK_ID_CURR').size()\n",
    "            prev_agg[f'PREV_STATUS_{status.upper()}_COUNT'] = status_count\n",
    "        \n",
    "        # Approved applications\n",
    "        approved = prev[prev['NAME_CONTRACT_STATUS'] == 'Approved'].groupby('SK_ID_CURR').size()\n",
    "        prev_agg['PREV_APPROVED_COUNT'] = approved\n",
    "        \n",
    "        # Approval rate\n",
    "        prev_agg['PREV_APPROVAL_RATE'] = approved / prev_agg['PREV_APPLICATION_COUNT']\n",
    "    \n",
    "    # 3. Credit amount aggregations\n",
    "    if 'AMT_CREDIT' in prev.columns:\n",
    "        prev_agg.update({\n",
    "            'PREV_AMT_CREDIT_MEAN': prev.groupby('SK_ID_CURR')['AMT_CREDIT'].mean(),\n",
    "            'PREV_AMT_CREDIT_MAX': prev.groupby('SK_ID_CURR')['AMT_CREDIT'].max(),\n",
    "            'PREV_AMT_CREDIT_MIN': prev.groupby('SK_ID_CURR')['AMT_CREDIT'].min(),\n",
    "            'PREV_AMT_CREDIT_SUM': prev.groupby('SK_ID_CURR')['AMT_CREDIT'].sum(),\n",
    "        })\n",
    "    \n",
    "    # 4. Application amount aggregations\n",
    "    if 'AMT_APPLICATION' in prev.columns:\n",
    "        prev_agg.update({\n",
    "            'PREV_AMT_APPLICATION_MEAN': prev.groupby('SK_ID_CURR')['AMT_APPLICATION'].mean(),\n",
    "            'PREV_AMT_APPLICATION_MAX': prev.groupby('SK_ID_CURR')['AMT_APPLICATION'].max(),\n",
    "            'PREV_AMT_APPLICATION_SUM': prev.groupby('SK_ID_CURR')['AMT_APPLICATION'].sum(),\n",
    "        })\n",
    "        \n",
    "        # Application to credit ratio\n",
    "        if 'AMT_CREDIT' in prev.columns:\n",
    "            prev['APPLICATION_CREDIT_RATIO'] = prev['AMT_APPLICATION'] / (prev['AMT_CREDIT'] + 1)\n",
    "            prev_agg['PREV_APP_CREDIT_RATIO_MEAN'] = prev.groupby('SK_ID_CURR')['APPLICATION_CREDIT_RATIO'].mean()\n",
    "    \n",
    "    # 5. Annuity aggregations\n",
    "    if 'AMT_ANNUITY' in prev.columns:\n",
    "        prev_agg.update({\n",
    "            'PREV_AMT_ANNUITY_MEAN': prev.groupby('SK_ID_CURR')['AMT_ANNUITY'].mean(),\n",
    "            'PREV_AMT_ANNUITY_MAX': prev.groupby('SK_ID_CURR')['AMT_ANNUITY'].max(),\n",
    "            'PREV_AMT_ANNUITY_SUM': prev.groupby('SK_ID_CURR')['AMT_ANNUITY'].sum(),\n",
    "        })\n",
    "    \n",
    "    # 6. Days decision features\n",
    "    if 'DAYS_DECISION' in prev.columns:\n",
    "        prev_agg.update({\n",
    "            'PREV_DAYS_DECISION_MEAN': prev.groupby('SK_ID_CURR')['DAYS_DECISION'].mean(),\n",
    "            'PREV_DAYS_DECISION_MAX': prev.groupby('SK_ID_CURR')['DAYS_DECISION'].max(),\n",
    "            'PREV_DAYS_DECISION_MIN': prev.groupby('SK_ID_CURR')['DAYS_DECISION'].min(),\n",
    "        })\n",
    "    \n",
    "    # 7. Contract type features\n",
    "    if 'NAME_CONTRACT_TYPE' in prev.columns:\n",
    "        # Count of contract types\n",
    "        prev_agg['PREV_CONTRACT_TYPES_COUNT'] = prev.groupby('SK_ID_CURR')['NAME_CONTRACT_TYPE'].nunique()\n",
    "        \n",
    "        # Most common contract type\n",
    "        contract_counts = prev.groupby(['SK_ID_CURR', 'NAME_CONTRACT_TYPE']).size().reset_index(name='count')\n",
    "        most_common_contract = contract_counts.loc[contract_counts.groupby('SK_ID_CURR')['count'].idxmax()]\n",
    "        most_common_contract = most_common_contract.set_index('SK_ID_CURR')['NAME_CONTRACT_TYPE']\n",
    "        prev_agg['PREV_MAIN_CONTRACT_TYPE'] = most_common_contract\n",
    "    \n",
    "    # Create aggregated DataFrame\n",
    "    prev_features = pd.DataFrame(prev_agg).reset_index()\n",
    "    \n",
    "    print(f\"Created {len(prev_features.columns) - 1} previous application features\")\n",
    "    \n",
    "    # Merge with application data\n",
    "    result_df = application_df.merge(prev_features, on='SK_ID_CURR', how='left')\n",
    "    \n",
    "    # Fill missing values\n",
    "    prev_cols = [col for col in prev_features.columns if col != 'SK_ID_CURR']\n",
    "    for col in prev_cols:\n",
    "        if col in result_df.columns:\n",
    "            if result_df[col].dtype in ['float64', 'float32', 'float16']:\n",
    "                result_df[col] = result_df[col].fillna(0)\n",
    "            elif result_df[col].dtype == 'object':\n",
    "                result_df[col] = result_df[col].fillna('No Previous Apps')\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "# %%\n",
    "# Create previous application features\n",
    "print(\"Creating previous application features for training data...\")\n",
    "train_with_prev = create_previous_application_features(train_with_bureau, prev_app)\n",
    "\n",
    "print(\"\\nCreating previous application features for test data...\")\n",
    "test_with_prev = create_previous_application_features(test_with_bureau, prev_app)\n",
    "\n",
    "# %%\n",
    "# Show new previous application features\n",
    "prev_features = [col for col in train_with_prev.columns if col not in train_with_bureau.columns]\n",
    "print(f\"\\nNew previous application features created: {len(prev_features)}\")\n",
    "print(\"Sample previous application features:\")\n",
    "for feature in prev_features[:10]:\n",
    "    print(f\"  - {feature}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## ðŸ”§ Feature Selection and Cleaning\n",
    "\n",
    "# %%\n",
    "def clean_and_select_features(df, is_train=True):\n",
    "    \"\"\"Clean features and select important ones.\"\"\"\n",
    "    \n",
    "    print(f\"\\nCleaning and selecting features...\")\n",
    "    print(f\"Original shape: {df.shape}\")\n",
    "    \n",
    "    features_df = df.copy()\n",
    "    \n",
    "    # 1. Drop columns with too many missing values (>80%)\n",
    "    missing_threshold = 0.8\n",
    "    missing_ratios = features_df.isnull().sum() / len(features_df)\n",
    "    columns_to_drop = missing_ratios[missing_ratios > missing_threshold].index.tolist()\n",
    "    \n",
    "    if columns_to_drop:\n",
    "        features_df = features_df.drop(columns=columns_to_drop)\n",
    "        print(f\"  Dropped {len(columns_to_drop)} columns with >{missing_threshold*100:.0f}% missing values\")\n",
    "    \n",
    "    # 2. Drop constant columns\n",
    "    constant_columns = []\n",
    "    for col in features_df.columns:\n",
    "        if col not in ['SK_ID_CURR', 'TARGET']:\n",
    "            if features_df[col].nunique() <= 1:\n",
    "                constant_columns.append(col)\n",
    "    \n",
    "    if constant_columns:\n",
    "        features_df = features_df.drop(columns=constant_columns)\n",
    "        print(f\"  Dropped {len(constant_columns)} constant columns\")\n",
    "    \n",
    "    # 3. Handle missing values\n",
    "    for col in features_df.columns:\n",
    "        if col not in ['SK_ID_CURR', 'TARGET']:\n",
    "            if features_df[col].isnull().any():\n",
    "                if features_df[col].dtype in ['float64', 'float32', 'float16', 'int64', 'int32', 'int16', 'int8']:\n",
    "                    # Fill numeric columns with median\n",
    "                    features_df[col] = features_df[col].fillna(features_df[col].median())\n",
    "                elif features_df[col].dtype == 'object':\n",
    "                    # Fill categorical columns with mode\n",
    "                    features_df[col] = features_df[col].fillna(features_df[col].mode()[0])\n",
    "    \n",
    "    print(f\"  Filled missing values\")\n",
    "    \n",
    "    # 4. Encode categorical variables\n",
    "    categorical_cols = features_df.select_dtypes(include=['object']).columns.tolist()\n",
    "    \n",
    "    if categorical_cols:\n",
    "        print(f\"  Encoding {len(categorical_cols)} categorical columns\")\n",
    "        \n",
    "        # Label encoding for tree-based models\n",
    "        for col in categorical_cols:\n",
    "            if col not in ['SK_ID_CURR', 'TARGET']:\n",
    "                features_df[col] = features_df[col].astype('category').cat.codes\n",
    "    \n",
    "    # 5. Remove highly correlated features\n",
    "    if is_train and 'TARGET' in features_df.columns:\n",
    "        # Calculate correlation matrix\n",
    "        numeric_cols = features_df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        numeric_cols = [col for col in numeric_cols if col not in ['SK_ID_CURR', 'TARGET']]\n",
    "        \n",
    "        if numeric_cols:\n",
    "            corr_matrix = features_df[numeric_cols].corr().abs()\n",
    "            \n",
    "            # Select upper triangle of correlation matrix\n",
    "            upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "            \n",
    "            # Find features with correlation greater than 0.9\n",
    "            to_drop = [column for column in upper.columns if any(upper[column] > 0.9)]\n",
    "            \n",
    "            if to_drop:\n",
    "                features_df = features_df.drop(columns=to_drop)\n",
    "                print(f\"  Dropped {len(to_drop)} highly correlated features\")\n",
    "    \n",
    "    print(f\"Final shape: {features_df.shape}\")\n",
    "    \n",
    "    return features_df\n",
    "\n",
    "# %%\n",
    "# Clean and select features for training data\n",
    "print(\"Processing training data...\")\n",
    "train_final = clean_and_select_features(train_with_prev, is_train=True)\n",
    "\n",
    "print(\"\\nProcessing test data...\")\n",
    "test_final = clean_and_select_features(test_with_prev, is_train=False)\n",
    "\n",
    "# Ensure test data has same columns as train data (except TARGET)\n",
    "train_cols = [col for col in train_final.columns if col != 'TARGET']\n",
    "test_final = test_final[train_cols + ['SK_ID_CURR']]\n",
    "\n",
    "# %% [markdown]\n",
    "# ## ðŸ“Š Feature Analysis\n",
    "\n",
    "# %%\n",
    "# Analyze feature importance with simple correlation\n",
    "if 'TARGET' in train_final.columns:\n",
    "    print(\"Analyzing feature correlations with target...\")\n",
    "    \n",
    "    # Calculate correlation with target\n",
    "    numeric_cols = train_final.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    numeric_cols = [col for col in numeric_cols if col not in ['SK_ID_CURR', 'TARGET']]\n",
    "    \n",
    "    correlations = []\n",
    "    for col in numeric_cols:\n",
    "        corr = train_final[col].corr(train_final['TARGET'])\n",
    "        correlations.append((col, abs(corr)))\n",
    "    \n",
    "    # Sort by absolute correlation\n",
    "    correlations.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    print(f\"\\nTop 10 features most correlated with TARGET:\")\n",
    "    for col, corr in correlations[:10]:\n",
    "        print(f\"  {col}: {corr:.4f}\")\n",
    "    \n",
    "    print(f\"\\nBottom 10 features least correlated with TARGET:\")\n",
    "    for col, corr in correlations[-10:]:\n",
    "        print(f\"  {col}: {corr:.4f}\")\n",
    "    \n",
    "    # Plot top correlated features\n",
    "    top_features = [col for col, _ in correlations[:20]]\n",
    "    \n",
    "    if top_features:\n",
    "        fig, axes = plt.subplots(5, 4, figsize=(20, 15))\n",
    "        axes = axes.flatten()\n",
    "        \n",
    "        for i, feature in enumerate(top_features[:20]):\n",
    "            if i < len(axes):\n",
    "                # Plot distribution by target\n",
    "                for target_val in [0, 1]:\n",
    "                    subset = train_final[train_final['TARGET'] == target_val][feature].dropna()\n",
    "                    axes[i].hist(subset, alpha=0.5, label=f'Target={target_val}', bins=30)\n",
    "                \n",
    "                axes[i].set_title(f'{feature}\\nCorr: {correlations[i][1]:.3f}')\n",
    "                axes[i].set_xlabel('Value')\n",
    "                axes[i].set_ylabel('Frequency')\n",
    "                axes[i].legend()\n",
    "                axes[i].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# %% [markdown]\n",
    "# ## ðŸ’¾ Save Engineered Features\n",
    "\n",
    "# %%\n",
    "# Save features to disk\n",
    "print(\"\\nSaving engineered features...\")\n",
    "\n",
    "# Create features directory\n",
    "import os\n",
    "os.makedirs('../features', exist_ok=True)\n",
    "\n",
    "# Save training features\n",
    "train_final.to_pickle('../features/train_features.pkl')\n",
    "print(f\"Training features saved: {train_final.shape}\")\n",
    "\n",
    "# Save test features\n",
    "test_final.to_pickle('../features/test_features.pkl')\n",
    "print(f\"Test features saved: {test_final.shape}\")\n",
    "\n",
    "# Save feature list\n",
    "feature_list = [col for col in train_final.columns if col not in ['SK_ID_CURR', 'TARGET']]\n",
    "with open('../features/feature_list.txt', 'w') as f:\n",
    "    for feature in feature_list:\n",
    "        f.write(f\"{feature}\\n\")\n",
    "\n",
    "print(f\"Feature list saved: {len(feature_list)} features\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## ðŸ“ Summary\n",
    "\n",
    "# %%\n",
    "print(\"=\" * 60)\n",
    "print(\"FEATURE ENGINEERING SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\n1. APPLICATION FEATURES:\")\n",
    "print(f\"   - Original: {len(app_train.columns)} columns\")\n",
    "print(f\"   - New: {len(app_train_features.columns)} columns\")\n",
    "print(f\"   - Created: {len(app_train_features.columns) - len(app_train.columns)} new features\")\n",
    "\n",
    "print(f\"\\n2. BUREAU FEATURES:\")\n",
    "print(f\"   - Added: {len(bureau_features)} bureau features\")\n",
    "\n",
    "print(f\"\\n3. PREVIOUS APPLICATION FEATURES:\")\n",
    "print(f\"   - Added: {len(prev_features)} previous application features\")\n",
    "\n",
    "print(f\"\\n4. FINAL DATASETS:\")\n",
    "print(f\"   - Training: {train_final.shape[0]} samples, {train_final.shape[1]} features\")\n",
    "print(f\"   - Test: {test_final.shape[0]} samples, {test_final.shape[1]} features\")\n",
    "\n",
    "print(f\"\\n5. NEXT STEPS:\")\n",
    "print(f\"   - Model training with {len(feature_list)} features\")\n",
    "print(f\"   - Feature importance analysis\")\n",
    "print(f\"   - Hyperparameter tuning\")\n",
    "\n",
    "# Save summary\n",
    "summary = {\n",
    "    'train_samples': train_final.shape[0],\n",
    "    'train_features': train_final.shape[1],\n",
    "    'test_samples': test_final.shape[0],\n",
    "    'test_features': test_final.shape[1],\n",
    "    'feature_list_size': len(feature_list)\n",
    "}\n",
    "\n",
    "import json\n",
    "with open('../features/engineering_summary.json', 'w') as f:\n",
    "    json.dump(summary, f, indent=4)\n",
    "\n",
    "print(\"\\nSummary saved to features/engineering_summary.json\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
